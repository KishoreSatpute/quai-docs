"use strict";(self.webpackChunkquai_docs=self.webpackChunkquai_docs||[]).push([[3364],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>p});var i=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=i.createContext({}),l=function(e){var t=i.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=l(e.components);return i.createElement(c.Provider,{value:t},e.children)},d="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},m=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,c=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),d=l(n),m=a,p=d["".concat(c,".").concat(m)]||d[m]||h[m]||r;return n?i.createElement(p,o(o({ref:t},u),{},{components:n})):i.createElement(p,o({ref:t},u))}));function p(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,o=new Array(r);o[0]=m;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[d]="string"==typeof e?e:a,o[1]=s;for(var l=2;l<r;l++)o[l]=n[l];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}m.displayName="MDXCreateElement"},5353:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var i=n(7462),a=(n(7294),n(3905));const r={title:"Latency",description:"How latency is minimized within Quai Network.",slug:"/latency",hide_table_of_contents:!1,sidebar_position:2},o="Latency",s={unversionedId:"learn/advanced-introduction/hierarchical-structure/latency",id:"learn/advanced-introduction/hierarchical-structure/latency",title:"Latency",description:"How latency is minimized within Quai Network.",source:"@site/docs/learn/advanced-introduction/hierarchical-structure/latency.md",sourceDirName:"learn/advanced-introduction/hierarchical-structure",slug:"/latency",permalink:"/latency",draft:!1,editUrl:"https://github.com/dominant-strategies/quai-docs/tree/main/docs/learn/advanced-introduction/hierarchical-structure/latency.md",tags:[],version:"current",lastUpdatedBy:"Juuddi",lastUpdatedAt:1692116968,formattedLastUpdatedAt:"Aug 15, 2023",sidebarPosition:2,frontMatter:{title:"Latency",description:"How latency is minimized within Quai Network.",slug:"/latency",hide_table_of_contents:!1,sidebar_position:2},sidebar:"learnSidebar",previous:{title:"Sharding",permalink:"/sharding"},next:{title:"Merged Mining",permalink:"/merged-mining"}},c={},l=[{value:"Networking Latencies",id:"networking-latencies",level:2},{value:"I/O Latencies",id:"io-latencies",level:2},{value:"Minimum Latency, Maximum Throughput",id:"minimum-latency-maximum-throughput",level:2}],u={toc:l},d="wrapper";function h(e){let{components:t,...r}=e;return(0,a.kt)(d,(0,i.Z)({},u,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"latency"},"Latency"),(0,a.kt)("p",null,"Latency is the primary factor limiting the throughput of a distributed ledger technology. Latency is the time it takes for a message or request to propagate to all nodes in the network. It is a measure of the responsiveness of a system and can be affected by various factors, such as network congestion, distance between the sender and receiver, and the complexity of the message or request."),(0,a.kt)("p",null,"High latency in a hash-based system directly leads to economic costs that affect the security of the network. Miners with higher latency are more likely to be inconsistent at the point of consensus (i.e. when a block is found), and thus more likely to find an uncle block, wasting the energy expended to create the block. Since energy is a scarce, economically valuable resource, high latency has a high economic cost to the security of a network. Low latency, on the other hand, minimizes wasted energy and facilitates smooth coordination across the network.\nThere are three primary kinds of latencies that impact overall latency: networking latencies, computation latencies, and input/output (I/O) latencies. Network bandwidth is related to the inverse of the sum of these latencies. Thus, decreasing latency increases bandwidth."),(0,a.kt)("h2",{id:"networking-latencies"},"Networking Latencies"),(0,a.kt)("p",null,"Networking latencies have the largest impact on overall latency and throughput. Networking latencies refer to delays that are due to the transfer of data over a network. These delays are impacted by a variety of factors, such as the amount of data transmitted, the number of nodes in the network, the latency between nodes, and the number of required trips through the network."),(0,a.kt)("p",null,"Quai minimizes networking latency through the creation of optimized sub networks. Each miner is incentivized to self-select the lowest-latency sub networks to mine. Miners are incentivized to mine lower-latency chains, as a higher latency to peers directly lowers profits due to the increased likelihood of producing uncle blocks.\nMiners will geographically organize over time due to this incentive, minimizing latency inside each sub network. Providing an initial suggestion for the geographic organization of miners will accelerate the convergence of miners into optimized sets."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Optimized Sub Networks",src:n(1478).Z,width:"3871",height:"2209"})),(0,a.kt)("p",null,"Quai's utilization of Proof-of-Entropy-Minima consensus further lowers network latency. PoEM ensures that only a single proof ever needs to be propagated throughout the network to achieve consensus. In contrast, Proof-of-Work requires a second proof to be shared before the network can reach consensus in a fork scenario. Proof-of-Stake consensus necessitates multiple rounds of propagation for each block to achieve consensus, as each proof must collect a specific number of social approvals to be considered valid. PoEM is an inherently faster method of reaching consensus than any alternative, as it minimizes both the amount of data transmitted and the number of trips that data must take through the network, thus optimizing network latency."),(0,a.kt)("h2",{id:"io-latencies"},"I/O Latencies"),(0,a.kt)("p",null,"I/O latencies refer to the delays that occur during the input/output operations of the network. These delays occur most often when a node is reading or writing data to its local storage. I/O latencies are impacted by the number of read/writes, the speed of storage devices, and state size. Quai Network will minimize I/O latencies by minimizing state size via state trimming, and optimizing node storage for RAM."),(0,a.kt)("p",null,"The larger the state of the network becomes, the more of an impact I/O latencies will have on overall throughput. Quai will keep state size manageable through the implementation of state trimming."),(0,a.kt)("p",null,"The fastest way for a node to query its state is to hold the state in random access memory (RAM). A node can query state from RAM more than 100x faster than it can query information from disk. Quai intends to optimize node caching to maximize the proportion of RAM queries and minimize the number of times that a node must slow down to query disk."),(0,a.kt)("h2",{id:"minimum-latency-maximum-throughput"},"Minimum Latency, Maximum Throughput"),(0,a.kt)("p",null,"Maximum total network bandwidth, and thus throughput, can be achieved by minimizing overall network latency. By addressing the three primary contributors to overall network latency (networking latencies, computation latencies, and I/O latencies) Quai Network creates a highly performant network capable of processing upwards of 50,000 TPS."))}h.isMDXComponent=!0},1478:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/LatencyMap-1097ed3be692635cc455c422dc38dc86.png"}}]);